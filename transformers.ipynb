{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shout out to http://peterbloem.nl/blog/transformers for simplifing the OG paper https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset idea from https://github.com/karpathy/minGPT/blob/7569ab9d7fc476a783619d56fec10e0a4c8afdd6/play_math.ipynb\n",
    "class AdditionDataModule(pl.LightningDataModule):\n",
    "\t\"\"\"\n",
    "\tReturns addition problems of up to some number of digits in the inputs. Recall\n",
    "\tthat all GPT cares about are sequences of integers, and completing them according to\n",
    "\tpatterns in the data. Therefore, we have to somehow encode addition problems\n",
    "\tas a sequence of integers.\n",
    "\n",
    "\tThe sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
    "\tencoding will simply be the n-digit first number, n-digit second number, \n",
    "\tand (n+1)-digit result, all simply concatenated together. Because each addition\n",
    "\tproblem is so structured, there is no need to bother the model with encoding\n",
    "\t+, =, or other tokens. Each possible sequence has the same length, and simply\n",
    "\tcontains the raw digits of the addition problem.\n",
    "\n",
    "\tAs a few examples, the 2-digit problems:\n",
    "\t- 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
    "\t- 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
    "\tetc.\n",
    "\n",
    "\tWe will also only train GPT on the final (n+1)-digits because the first\n",
    "\ttwo n-digits are always assumed to be given. So when we give GPT an exam later,\n",
    "\twe will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
    "\tto add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n",
    "\tin 3 sequential steps.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, batch_size=32, split=0.8):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.ds_X, self.ds_Y = self.get_dataset()\n",
    "\t\tshuffler = np.random.permutation(self.ds_X.shape[0])\n",
    "\t\tself.ds_X = self.ds_X[shuffler]\n",
    "\t\tself.ds_Y = self.ds_Y[shuffler]\n",
    "\t\tself.split = int(self.ds_X.shape[0]*split)\n",
    "\t\tself.batch_size = batch_size\n",
    "\n",
    "\tdef get_dataset(self):\n",
    "\t\tret = []\n",
    "\t\tfor i in range(100):\n",
    "\t\t\tfor j in range(100):\n",
    "\t\t\t\ts = i+j\n",
    "\t\t\tret.append([i//10, i%10, j//10, j%10, s//100, (s//10)%10, s%10])\n",
    "\t\tds = np.array(ret)\n",
    "\t\treturn ds[:, 0:6], np.copy(ds[:, 1:])  \n",
    "\n",
    "\tdef train_dataloader(self):\n",
    "\t\tds_X_train, ds_Y_train = self.ds_X[0:self.split], self.ds_Y[0:self.split]\n",
    "\t\treturn torch.utils.data.DataLoader(list(zip(ds_X_train, ds_Y_train)), \\\n",
    "\t\t\tnum_workers=16, \\\n",
    "\t\t\tbatch_size=self.batch_size)\n",
    "\n",
    "\tdef val_dataloader(self):\n",
    "\t\tds_X_test, ds_Y_test = self.ds_X[self.split:], self.ds_Y[self.split:]\n",
    "\t\treturn torch.utils.data.DataLoader(list(zip(ds_X_test, ds_Y_test)), \\\n",
    "\t\t\tnum_workers=16, \\\n",
    "\t\t\tbatch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attn(q, k, v):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    return torch.matmul(attention, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.W_qkv = nn.Linear(embed_dim, 3*embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, input_dim = x.size()\n",
    "        assert input_dim == self.embed_dim # sanity check\n",
    "        qkv = self.W_qkv(x)\n",
    "        \n",
    "        # Separate Q, K, V from stacked linear output\n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # Determine output values\n",
    "        x = scaled_dot_product_attn(q, k, v)\n",
    "        x = x.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
    "        x = x.reshape(batch_size, seq_length, self.embed_dim)\n",
    "        x = self.W_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attention layer\n",
    "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Two-layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Layers to apply in between the main layers\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x,):\n",
    "        # Attention part\n",
    "        x = self.norm1(x + self.dropout(self.self_attn(x)))\n",
    "        # MLP part\n",
    "        x = self.norm2(x + self.dropout(self.linear_net(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "            d_model - Hidden dimensionality of the input.\n",
    "            max_len - Maximum length of a sequence to expect.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        self.pe = torch.zeros(max_len, d_model, device=device)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
    "        # Used for tensors that need to be on the same device as the module.\n",
    "        self.register_buffer('PositionalEncoding', self.pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embed_dim=128, num_classes=10, num_heads=4, num_layers=2, lr=3e-4, dim_feedforward=32, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Hidden dimensionality to use inside the Transformer\n",
    "            num_classes - Number of classes to predict per sequence element\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
    "            num_layers - Number of encoder blocks to use.\n",
    "            lr - Learning rate in the optimizer\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout to apply inside the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Embedding(num_classes, embed_dim),\n",
    "            PositionalEncoding(d_model=embed_dim),\n",
    "            *[EncoderBlock(embed_dim, num_heads, dim_feedforward) for x in range(num_layers)],\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        loss = F.nll_loss(output.view(-1, self.num_classes), y.view(-1))\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        pred = self.model(x).argmax(dim=2)\n",
    "        val_accuracy = (pred == y).type(torch.float).mean()\n",
    "        self.log(\"val_accuracy\", val_accuracy, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishab/anaconda3/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:446: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/rishab/Documents/codes/model-arch-implementation/quick-model-archs/transformers/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 169 K \n",
      "-------------------------------------\n",
      "169 K     Trainable params\n",
      "0         Non-trainable params\n",
      "169 K     Total params\n",
      "0.677     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3/3 [00:01<00:00,  2.67it/s, loss=-0.645, v_num=0, val_accuracy=0.325]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 3/3 [00:01<00:00,  2.63it/s, loss=-0.645, v_num=0, val_accuracy=0.325]\n"
     ]
    }
   ],
   "source": [
    "data = AdditionDataModule(batch_size=64)\n",
    "model = TransformerPredictor()\n",
    "trainer = pl.Trainer(enable_progress_bar=True, max_epochs=5, gpus=1, log_every_n_steps=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52c1536be2703022bdc2410894b56c577b65ee369faba254cdff0f9aec6fe983"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
