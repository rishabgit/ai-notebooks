{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1337"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "import torchmetrics\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "seed_everything(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_consecutive_frames = 10\n",
    "\n",
    "class TwoStreamActionNet(pl.LightningModule):\n",
    "    '''\n",
    "    https://arxiv.org/abs/1406.2199\n",
    "    '''\n",
    "    def __init__(self, num_classes=101):\n",
    "        super().__init__()\n",
    "        # for single RGB frame RGB\n",
    "        self.spatial_stream_net = models.resnet18(weights='IMAGENET1K_V1') # any convnet works\n",
    "        self.spatial_stream_net.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        # for multi-frame optical flow, hard coding to 10 according to paper examples\n",
    "        self.temporal_stream_net = models.resnet18()\n",
    "        self.temporal_stream_net.conv1 = nn.Conv2d(L_consecutive_frames*2, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.temporal_stream_net.fc = nn.Linear(in_features=512, out_features=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.acc = torchmetrics.Accuracy()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, x_rgb, x_optical_flow):\n",
    "        x_rgb = self.spatial_stream_net(x_rgb)\n",
    "        x_optical_flow = self.temporal_stream_net(x_optical_flow)\n",
    "        y_hat = self.softmax(x_rgb + x_optical_flow)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x_rgb, x_optical_flow), y = batch\n",
    "        x_rgb = self.spatial_stream_net(x_rgb)\n",
    "        x_optical_flow = self.temporal_stream_net(x_optical_flow)\n",
    "        y_hat = self.softmax(x_rgb + x_optical_flow)\n",
    "        train_loss = self.loss(y_hat, y)\n",
    "        train_acc = self.acc(y_hat, y)\n",
    "        self.log_dict({\"train_acc\": train_acc, \"train_loss\": train_loss}, prog_bar=True)\n",
    "        return train_loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x_rgb, x_optical_flow), y = batch\n",
    "        x_rgb = self.spatial_stream_net(x_rgb)\n",
    "        x_optical_flow = self.temporal_stream_net(x_optical_flow)\n",
    "        y_hat = self.softmax(x_rgb + x_optical_flow)\n",
    "        val_loss = self.loss(y_hat, y)\n",
    "        val_acc = self.acc(y_hat, y)\n",
    "        self.log_dict({\"val_acc\": val_acc, \"val_loss\": val_loss}, prog_bar=True)\n",
    "        return val_loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # lr and schedular as written in paper\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=1e-02, momentum=0.9)\n",
    "        # wasn't sure which lr scheduler would be best fit, so picked steplr\n",
    "        # what should be according to paper: 1e-03 from 50k iters, 1e-04 from 70k iters\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "        return {\n",
    "            \"optimizer\":optimizer,\n",
    "            \"lr_scheduler\" : {\n",
    "                \"scheduler\" : scheduler,\n",
    "                \"monitor\" : \"train_loss\",\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short ucf101 downloaded from here- https://www.kaggle.com/datasets/nguyntindng0506/ucf101\n",
    "\n",
    "class TwoStreamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/datasets/nguyntindng0506/ucf101\n",
    "    Args:\n",
    "        dataset (str): Name of dataset. Defaults to 'ucf101'.\n",
    "        split (str): Determines which folder of the directory the dataset will read from. Defaults to 'train'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset='data', split='train'):\n",
    "        self.root_dir = dataset\n",
    "        self.split = split\n",
    "\n",
    "        # Section 5 \"Implementation details\"\n",
    "        rescale_size = 256\n",
    "        crop_size = 224\n",
    "\n",
    "        if split == 'train':\n",
    "            self.h_flip = transforms.RandomHorizontalFlip(p=.5)\n",
    "            self.color_jitter = transforms.ColorJitter(brightness=.5, hue=.5, \\\n",
    "                    contrast=.5, saturation=.5)\n",
    "        self.spatial_img_tfs = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((rescale_size, rescale_size)),\n",
    "                    transforms.RandomCrop((crop_size, crop_size)),\n",
    "                ])\n",
    "\n",
    "        self.temporal_tfs = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize((rescale_size, rescale_size)), # not mentioned but few vids h,w < 224\n",
    "                    transforms.RandomCrop((crop_size, crop_size)),\n",
    "                ])\n",
    "\n",
    "        print('Loading and processing videos...')\n",
    "        self.rgb_frames, self.optical_flow_stacks, labels = [], [], []\n",
    "        # load videos now - loading/processing vids during training ate up a lot of time\n",
    "        # unsure if loading/processing everything NOW makes sense either\n",
    "        # especially when dataset size is big\n",
    "        for fname in tqdm(sorted(os.listdir(os.path.join(self.root_dir, split)))):\n",
    "            rgb_frame, optical_flow_stacked = self.process_video(os.path.join(self.root_dir, split, fname))\n",
    "            if optical_flow_stacked.shape[-1] == 20 and rgb_frame is not None: \n",
    "                self.rgb_frames.append(rgb_frame)\n",
    "                self.optical_flow_stacks.append(optical_flow_stacked)\n",
    "                labels.append(fname.split(\"_\")[1])\n",
    "\n",
    "        print(f'Size of {split}: {len(self.rgb_frames)}')\n",
    "        print(f'Available label classes: {sorted(list(set(labels)))}')\n",
    "\n",
    "        # Prepare a mapping between the label names (strings) and indices (ints)\n",
    "        self.label2index = {label: index for index, label in enumerate(sorted(set(labels)))}\n",
    "        # Convert the list of label names into an array of label indices\n",
    "        self.label_array = np.array([self.label2index[label] for label in labels], dtype=int)\n",
    "        assert len(self.label_array) == len(self.rgb_frames) == len(self.optical_flow_stacks)\n",
    "\n",
    "\n",
    "    def num_classes(self):\n",
    "        return len(list(set(self.label_array)))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_frames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        rgb = self.spatial_img_tfs(self.rgb_frames[index])\n",
    "        optical_stacks = self.temporal_tfs(self.optical_flow_stacks[index])\n",
    "        if self.split == 'train':\n",
    "            rgb = self.h_flip(rgb)\n",
    "            rgb = self.color_jitter(rgb)\n",
    "            optical_stacks = self.h_flip(optical_stacks)\n",
    "        # paper doesn't say anything about normalization\n",
    "        # RGB frame gets this op automatically\n",
    "        optical_stacks = optical_stacks / 255. # definitely not correct number\n",
    "        labels = np.array(self.label_array[index])\n",
    "        return (rgb, optical_stacks), torch.from_numpy(labels).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "    def process_video(self, fname):\n",
    "        optical_flow_frames = []\n",
    "        capture = cv2.VideoCapture(fname)\n",
    "        frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        count = 0\n",
    "        random_frame_rgb_idx = np.random.randint(1, frame_count)\n",
    "        random_optical_start_idx = np.random.randint(0, frame_count - L_consecutive_frames)\n",
    "        prev_frame = None\n",
    "        rgb_frame = None\n",
    "\n",
    "        while capture.isOpened():\n",
    "            ret, frame = capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            count += 1\n",
    "            if count == random_frame_rgb_idx:\n",
    "                rgb_frame = frame\n",
    "            if count >= random_optical_start_idx and len(optical_flow_frames) < 2*L_consecutive_frames:\n",
    "                if prev_frame is None:\n",
    "                    prev_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                else:\n",
    "                    # converting to gray should make optical flow calc cheaper\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                    flow = cv2.calcOpticalFlowFarneback(prev_frame, frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "                    prev_frame = frame\n",
    "                    # 3.1 \"ConvNet input configurations\"\n",
    "                    # Not 100% sure if this is \"Optical flow stacking\"\n",
    "                    optical_flow_frames.append(flow[:, :, 0])\n",
    "                    optical_flow_frames.append(flow[:, :, 1])\n",
    "        capture.release()\n",
    "        # technically stack size is the channel for model \n",
    "        # transposing now for cleaner code?\n",
    "        optical_flow_frames = np.array(optical_flow_frames).transpose((1, 2, 0)) \n",
    "        return rgb_frame, optical_flow_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing videos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ac605c33f94a2286d47fccab2f662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test: 224\n",
      "Available label classes: ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n"
     ]
    }
   ],
   "source": [
    "val_set = TwoStreamDataset('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing videos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbd0d87c040405bb1e0569fe2e89684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train: 594\n",
      "Available label classes: ['CricketShot', 'PlayingCello', 'Punch', 'ShavingBeard', 'TennisSwing']\n"
     ]
    }
   ],
   "source": [
    "train_set = TwoStreamDataset('data', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 224, 5, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set), train_set.num_classes(), val_set.num_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoStreamActionNet(num_classes=train_set.num_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TwoStreamActionNet                            [10, 5]                   --\n",
       "├─ResNet: 1-1                                 [10, 5]                   --\n",
       "│    └─Conv2d: 2-1                            [10, 64, 112, 112]        9,408\n",
       "│    └─BatchNorm2d: 2-2                       [10, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-3                              [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-4                         [10, 64, 56, 56]          --\n",
       "│    └─Sequential: 2-5                        [10, 64, 56, 56]          --\n",
       "│    │    └─BasicBlock: 3-1                   [10, 64, 56, 56]          73,984\n",
       "│    │    └─BasicBlock: 3-2                   [10, 64, 56, 56]          73,984\n",
       "│    └─Sequential: 2-6                        [10, 128, 28, 28]         --\n",
       "│    │    └─BasicBlock: 3-3                   [10, 128, 28, 28]         230,144\n",
       "│    │    └─BasicBlock: 3-4                   [10, 128, 28, 28]         295,424\n",
       "│    └─Sequential: 2-7                        [10, 256, 14, 14]         --\n",
       "│    │    └─BasicBlock: 3-5                   [10, 256, 14, 14]         919,040\n",
       "│    │    └─BasicBlock: 3-6                   [10, 256, 14, 14]         1,180,672\n",
       "│    └─Sequential: 2-8                        [10, 512, 7, 7]           --\n",
       "│    │    └─BasicBlock: 3-7                   [10, 512, 7, 7]           3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [10, 512, 7, 7]           4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [10, 512, 1, 1]           --\n",
       "│    └─Linear: 2-10                           [10, 5]                   2,565\n",
       "├─ResNet: 1-2                                 [10, 5]                   2,565\n",
       "│    └─Conv2d: 2-11                           [10, 64, 112, 112]        62,720\n",
       "│    └─BatchNorm2d: 2-12                      [10, 64, 112, 112]        128\n",
       "│    └─ReLU: 2-13                             [10, 64, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-14                        [10, 64, 56, 56]          --\n",
       "│    └─Sequential: 2-15                       [10, 64, 56, 56]          --\n",
       "│    │    └─BasicBlock: 3-9                   [10, 64, 56, 56]          73,984\n",
       "│    │    └─BasicBlock: 3-10                  [10, 64, 56, 56]          73,984\n",
       "│    └─Sequential: 2-16                       [10, 128, 28, 28]         --\n",
       "│    │    └─BasicBlock: 3-11                  [10, 128, 28, 28]         230,144\n",
       "│    │    └─BasicBlock: 3-12                  [10, 128, 28, 28]         295,424\n",
       "│    └─Sequential: 2-17                       [10, 256, 14, 14]         --\n",
       "│    │    └─BasicBlock: 3-13                  [10, 256, 14, 14]         919,040\n",
       "│    │    └─BasicBlock: 3-14                  [10, 256, 14, 14]         1,180,672\n",
       "│    └─Sequential: 2-18                       [10, 512, 7, 7]           4,720,640\n",
       "│    │    └─BasicBlock: 3-15                  [10, 512, 7, 7]           3,673,088\n",
       "├─Accuracy: 1-3                               --                        --\n",
       "├─CrossEntropyLoss: 1-4                       --                        --\n",
       "├─ResNet: 1                                   --                        --\n",
       "│    └─Sequential: 2                          --                        --\n",
       "│    │    └─BasicBlock: 3-16                  [10, 512, 7, 7]           4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-19                [10, 512, 1, 1]           --\n",
       "│    └─Linear: 2-20                           [10, 5]                   2,565\n",
       "├─Softmax: 1-5                                [10, 5]                   --\n",
       "===============================================================================================\n",
       "Total params: 22,411,466\n",
       "Trainable params: 22,411,466\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 42.96\n",
       "===============================================================================================\n",
       "Input size (MB): 46.16\n",
       "Forward/backward pass size (MB): 794.79\n",
       "Params size (MB): 89.65\n",
       "Estimated Total Size (MB): 930.60\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, [(10, 3, 224, 224),(10, 20, 224, 224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(val_set, batch_size=4, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 224, 224]),\n",
       " torch.Size([4, 20, 224, 224]),\n",
       " torch.Size([4]),\n",
       " tensor([2, 1, 4, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "x[0].shape, x[1].shape, y.shape, y.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=4,accelerator='gpu', devices=1, log_every_n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lightning_logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/rishab/Documents/codes/model-arch-implementation/quick-model-archs/two-stream-action-recog/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | spatial_stream_net  | ResNet           | 11.2 M\n",
      "1 | temporal_stream_net | ResNet           | 11.2 M\n",
      "2 | softmax             | Softmax          | 0     \n",
      "3 | acc                 | Accuracy         | 0     \n",
      "4 | loss                | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------------\n",
      "22.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.646    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 156/156 [00:09<00:00, 17.08it/s, loss=1.73, v_num=0, train_acc=0.250, train_loss=1.650, val_acc=0.219, val_loss=1.690]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 156/156 [00:09<00:00, 16.38it/s, loss=1.73, v_num=0, train_acc=0.250, train_loss=1.650, val_acc=0.219, val_loss=1.690]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "# run 'tensorboard --logdir .' on terminal for logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52c1536be2703022bdc2410894b56c577b65ee369faba254cdff0f9aec6fe983"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
